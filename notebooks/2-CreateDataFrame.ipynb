{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd2f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa75a43",
   "metadata": {},
   "source": [
    "1. The title of the thread CHECK\n",
    "2. The subreddit that the thread corresponds to CHECK\n",
    "3. The length of time it has been up on Reddit\n",
    "4. The number of comments on the thread CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea30c9",
   "metadata": {},
   "source": [
    "# Features\n",
    "----\n",
    "### Required:\n",
    "- `title`\n",
    "- `subreddit`\n",
    "- `num_comments`\n",
    "- `created_utc`\n",
    "- `pulled_time`\n",
    "- `time_on_reddit` (created time minus pulled time)\n",
    "\n",
    "### Additional Options\n",
    "- `selftext`\n",
    "- `post_hint` (needed to determine if the thread contains text or only title, image, link or video)\n",
    "\n",
    "### Possibly Interesting\n",
    "- `author`\n",
    "- `subreddit_subscribers`\n",
    "- `num_crossposts`\n",
    "- `over_18`\n",
    "- `ups`\n",
    "- `edited`\n",
    "- `parent_whitelist_status`\n",
    "- `locked`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c4a04",
   "metadata": {},
   "source": [
    "### Useless Features\n",
    "----\n",
    "Some features were particularly uneccessary\n",
    "\n",
    "| feature | reason |\n",
    "| --------| ------ |\n",
    "| media_only | all False |\n",
    "| pinned | all False |\n",
    "| downs | all False |\n",
    "| is_created_from_ads_ui | all False |\n",
    "| discussion_type | all None |\n",
    "\n",
    "### Skewed Features\n",
    "----\n",
    "These features could be useful but their skew needs to be considered\n",
    "\n",
    "| feature | reason |\n",
    "| --------| ------ |\n",
    "| stickied | highly skewed to False |\n",
    "| locked | highly skewed to False |\n",
    "| author | unlikely to be many duplicates |\n",
    "| edited | only ~7.5% edited |\n",
    "| author_cakeday | only ~0.49% cakedays |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5db350",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_FEATURES = ['subreddit', 'title', 'author', 'num_comments', 'created_utc', 'pulled_time',\n",
    "                  'selftext', 'post_hint', 'subreddit_subscribers', 'num_crossposts', 'over_18',\n",
    "                  'id', 'is_video', 'parent_whitelist_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cdbd30",
   "metadata": {},
   "source": [
    "# Functions\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c9205",
   "metadata": {},
   "source": [
    "### Data Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de799218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threads(folder, file):\n",
    "    \"\"\"Returns all threads from a specific file.\n",
    "    Function also adds a time-stamp to the pulled data in the format YYYY-MM-DD-HH\"\"\"\n",
    "    \n",
    "    #Delcare a blank threads list which will be appended and returned\n",
    "    threads = []\n",
    "    \n",
    "    #Open file\n",
    "    with open(f\"{folder}/{file}\", \"r\") as f:\n",
    "        pages = json.load(f)\n",
    "        \n",
    "        #Each file has 80 pages\n",
    "        for page in pages:\n",
    "            #Pull threads from each page, ass a pull_time equal to the file name\n",
    "            for thread in page[\"data\"][\"children\"]:\n",
    "                thread[\"data\"][\"pulled_time\"] = file\n",
    "                threads.append(thread)\n",
    "    return threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e789339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_threads(data_folder=\"./data/json/\"):\n",
    "    \"\"\"Returns all threads from a specifed folder. By default it is ./data/json/\n",
    "    Function assumes files will be held in a folder for each date\"\"\"\n",
    "    #Declare a blank lists of threads to append to and return\n",
    "    all_threads = []\n",
    "    \n",
    "    #For each folder in ./data/json\n",
    "    for folder in os.listdir(data_folder):\n",
    "        \n",
    "        #read each file and append collected threads to all threads.\n",
    "        for file in os.listdir(f\"{data_folder}/{folder}\"):\n",
    "            all_threads += get_threads(f\"{data_folder}/{folder}\", file)\n",
    "    return all_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33db1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(threads, features):\n",
    "    \"\"\"This function will go through listed threads and check if a desired feature has\n",
    "    a value. If it does not have a value it will assign np.NaN. This ensures our data\n",
    "    all has the same shape when creating a DF.\"\"\"\n",
    "    \n",
    "    #Declare a blank list of all rows to be appended and used to create a DF\n",
    "    all_rows = []\n",
    "    \n",
    "    for thread in threads:\n",
    "        row = []\n",
    "        for feature in features:\n",
    "            #Try to append a feature to a row\n",
    "            try:\n",
    "                row.append(thread[\"data\"][feature])\n",
    "            #If it can't append it is because the thread doesn't have the feature.\n",
    "            #Append np.NaN\n",
    "            except:\n",
    "                row.append(np.NaN)\n",
    "        all_rows.append(row)\n",
    "        \n",
    "    #create and return a DF with rows all_rows and features as columns\n",
    "    return pd.DataFrame(all_rows, columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b557ea",
   "metadata": {},
   "source": [
    "### Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c05425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulled_time_to_datetime(pulled_time):\n",
    "    \"\"\"Converts the file names (pulled_time) to a DateTime Object.\n",
    "    This function is just for clarity. A lambda would work fine.\n",
    "    Side Note: I should have used timestamps when pulling. I now know for next time.\"\"\"\n",
    "    try:\n",
    "        components = pulled_time.split(\"-\")\n",
    "        year = int(components[0])\n",
    "        month = int(components[1])\n",
    "        day = int(components[2])\n",
    "        hour = int(components[3])\n",
    "        return datetime.datetime(year, month, day, hour)\n",
    "    \n",
    "    #If it fails to create a Datetime Object\n",
    "    except:\n",
    "        #Check if it already is a dateimte object\n",
    "        if isinstance(pulled_time, datetime.datetime):\n",
    "            return pulled_time\n",
    "        else:\n",
    "            return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47de37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alert(feature, dataframe, check_type=False):\n",
    "    \"\"\"Simple Exception Handler.\n",
    "    Prints the common exceptions expected when cleaning DF\"\"\"\n",
    "    \n",
    "    if feature not in dataframe.columns:\n",
    "        print(f'Error: \"{feature}\" not in feature set!')\n",
    "    elif check_type != False:\n",
    "        if isinstance(dataframe[feature].iloc[1], check_type):\n",
    "            print(f'Error: \"{feature}\" already a {check_type} object!')\n",
    "    else:\n",
    "        print(f'Unkown Error on \"{feature}\": Debug!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b908a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(dataframe):\n",
    "    \"\"\"Function cleans the DF and warns if features are not in the DF.\n",
    "    If run multiple times it will not overwrite existing changes and will instead\n",
    "    print a notification that something unexpected occured.\n",
    "    \n",
    "    NOTE: I plan on changing this. One of my peeves dealing with data is when\n",
    "    I inadvertantly do additional tranformations that break the data. I wanted\n",
    "    to basically detect if it was already cleaned before touching it but this\n",
    "    is way too messy and should just be a series of if conditions.\"\"\"\n",
    "    \n",
    "    #created_utc\n",
    "    try:\n",
    "        #created_utc is a timescape.Convert to Datetime\n",
    "        dataframe[\"created_utc\"] = dataframe[\"created_utc\"].map(\n",
    "                                   datetime.datetime.utcfromtimestamp)\n",
    "        #assign the Timezone UTC\n",
    "        dataframe[\"created_utc\"] = dataframe[\"created_utc\"].dt.tz_localize('UTC')\n",
    "        #Convert froM UTC to Eastern Time\n",
    "        dataframe[\"created_utc\"] = dataframe[\"created_utc\"].dt.tz_convert('US/Eastern')\n",
    "        #Remove Time Zone Info\n",
    "        dataframe[\"created_utc\"] = dataframe[\"created_utc\"].dt.tz_localize(None)\n",
    "    except:\n",
    "        alert(\"created_utc\", dataframe, check_type=datetime.datetime)\n",
    "\n",
    "    #pulled_time\n",
    "    try:\n",
    "        #pulled_time is a string. Add minutes and seconds. Convert to Datetime\n",
    "        dataframe[\"pulled_time\"] = dataframe[\"pulled_time\"].map(pulled_time_to_datetime)\n",
    "    except:\n",
    "        alert(\"pulled_time\", dataframe)\n",
    "        \n",
    "    try:\n",
    "        #This is the only feature engineering in this section\n",
    "        dataframe[\"time_on_reddit\"] = (dataframe[\"pulled_time\"] - dataframe[\"created_utc\"])\n",
    "        #convert to seconds\n",
    "        dataframe[\"time_on_reddit\"] = dataframe[\"time_on_reddit\"].dt.seconds\n",
    "    except:\n",
    "        #Feature won't be in DF until after at least one clean.\n",
    "        #If this is reported to not be in DF then created_utc or pulled_time is not\n",
    "        alert(\"time_on_reddit\", dataframe)\n",
    "    \n",
    "    #post hint\n",
    "    try:\n",
    "        #if post_hint is null, text, else post_hint\n",
    "        dataframe[\"post_hint\"] = np.where(dataframe[\"post_hint\"].isnull(),\n",
    "                                          \"text\",\n",
    "                                          dataframe[\"post_hint\"])\n",
    "    except:\n",
    "        alert(\"post_hint\", dataframe)\n",
    "        \n",
    "    #parent_whitelist_status\n",
    "    try:\n",
    "        #if post_hint is null, text, else post_hint\n",
    "        dataframe[\"parent_whitelist_status\"] = np.where(dataframe[\"parent_whitelist_status\"].isnull(),\n",
    "                                                        \"unknown\",\n",
    "                                                        dataframe[\"parent_whitelist_status\"])\n",
    "    except:\n",
    "        alert(\"parent_whitelist_status\", dataframe)\n",
    "    \n",
    "    #assign numerics to parent_whitelist_status\n",
    "    try:\n",
    "        value_dict = {\"all_ads\" : 3,\n",
    "                      \"some_ads\" : 2,\n",
    "                      \"no_ads\" : 1,\n",
    "                      \"unknown\" : 0}\n",
    "        dataframe[\"parent_whitelist_status\"] = dataframe[\"parent_whitelist_status\"].map(value_dict)\n",
    "    except:\n",
    "        alert(\"parent_whitelist_status\", dataframe)\n",
    "\n",
    "    #selftext\n",
    "    try:\n",
    "        #If selftext is \"\", None, else current text\n",
    "        dataframe[\"selftext\"] = np.where(dataframe[\"selftext\"] == \"\",\n",
    "                                         \"None\",\n",
    "                                         dataframe[\"selftext\"])     \n",
    "    except:\n",
    "        alert(\"selftext\", dataframe)\n",
    "        \n",
    "    dataframe[\"selftext\"] = np.where(dataframe[\"selftext\"].isnull(),\n",
    "                                     \"None\",\n",
    "                                     dataframe[\"selftext\"])\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd465d45",
   "metadata": {},
   "source": [
    "### Note:\n",
    "When possible adjust to improve efficiency based on Sophie's advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d675da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dataframe):\n",
    "    \"\"\"This is EXTREMELY inefficient. It takes over an hour to run.\n",
    "    I believe the solution is to use better pandas slections as well as\n",
    "    the mx feature. It works but it's definitely a brute force solution for now\"\"\"\n",
    "    \n",
    "    dups = dataframe[\"id\"].value_counts().index[dataframe[\"id\"].value_counts()>1]\n",
    "    dataframe[\"pulled_time\"] = pd.to_datetime(dataframe[\"pulled_time\"])\n",
    "    df = dataframe\n",
    "    \n",
    "    print(f\"Removing {len(dups)} duplicates from DataFrame.\")\n",
    "    for i, thread_id in enumerate(dups):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Removing {i}/{len(dups)} from DataFrame\")\n",
    "            print(f\"Current Time: {datetime.datetime.now()}\")\n",
    "        newest = datetime.datetime.min\n",
    "        keep = -2\n",
    "        \n",
    "        for i in dataframe[dataframe[\"id\"]==thread_id].index:\n",
    "            if dataframe.iloc[i][\"pulled_time\"] > newest:\n",
    "                keep = i\n",
    "                newest = dataframe.iloc[i][\"pulled_time\"]\n",
    "        arr = dataframe[dataframe[\"id\"]==thread_id].index\n",
    "        remove = np.delete(arr, np.where(arr == keep))\n",
    "        df = df[~df.index.isin(remove)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902964f5",
   "metadata": {},
   "source": [
    "### Side Use\n",
    "----\n",
    "Function not actively used in notebook. Exists to collect all features from imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407238c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_all_features(threads):\n",
    "    \"\"\"Simple feature to determine all functions in pulled threads.\n",
    "    This is only used to determine which features exist for EDA.\n",
    "    Should be used in conjunction with pull all threads.\"\"\"\n",
    "    features = []\n",
    "    for thread in threads:\n",
    "        features += thread[\"data\"].keys()\n",
    "    return list(set(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348403e3",
   "metadata": {},
   "source": [
    "## Main Function\n",
    "----\n",
    "#### Step 1: Get all information collected in the API pulls\n",
    "Collect all threads that were pulled by `pull_threads_executable.py`. The files are stored in `\"./data/json/\"`\n",
    "\n",
    "The raw data will not be included in full to the GitHub Reposistory as all of the files combined are quite large but a small portion will be included to show proof of concept. The file size is because I opted to pull and store all of the data from the API so I would have access to every possible feature during EDA. This actually worked out quite well as `post_hint` turned out to be incredibly relevant to determining whether the post type and I did not dicover this until late in the data collection process.\n",
    "\n",
    "#### Step 2: Create a Pandas DataFrame of selected features\n",
    "Iterate over all of the threads and ensure there is a value for every selected feature. If there is no value for a feature then assign np.NaN.\n",
    "\n",
    "#### Step 3: Clean the data\n",
    "There is not much data cleaning due to using the API. Cleaning Breakdown:\n",
    "- `created_utc` - Changed to `DateTime`. Convert to EST to align with `pulled_time`. Remove TimeZone\n",
    "- `pulled_time` - Correct the format to `YYYY-MM-DD HH:MM:SS` and change to `DateTime`\n",
    "- `time_on_reddit` - Feature Engineering.\n",
    "- `post_hint` - Change `np.NaN` to `text`\n",
    "- `parent_whitelist_status` - There are a few nulls in here. Change to `\"unkown\"`\n",
    "- `edited` - Assign `True` to edited threads instead of `Timestamp`\n",
    "- `author_cakeday` - change `np.NaN` to `False` if not the author's cakeday\n",
    "- `selftext` - Replace `np.NaN` with `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2b42ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    threads = get_all_threads()\n",
    "    thread_df = create_df(threads, FINAL_FEATURES)\n",
    "    thread_df = clean_df(thread_df)\n",
    "    thread_df = remove_duplicates(thread_df)\n",
    "    \n",
    "    thread_df.to_csv(\"../data/thread_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee379769",
   "metadata": {},
   "source": [
    "# Create DataFrame\n",
    "----\n",
    "***Note:*** *I have disabled the main function as the json files are too large for me to put on github.\n",
    "Some raw json files were included for testing purposes and a backup file is in /data in case information is overwritten*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9664be4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 39406 duplicates from DataFrame.\n",
      "Removing 0/39406 from DataFrame\n",
      "Current Time: 2022-01-08 20:37:42.508604\n",
      "Removing 10000/39406 from DataFrame\n",
      "Current Time: 2022-01-08 20:53:49.545806\n",
      "Removing 20000/39406 from DataFrame\n",
      "Current Time: 2022-01-08 21:08:28.567072\n",
      "Removing 30000/39406 from DataFrame\n",
      "Current Time: 2022-01-08 21:21:53.714634\n"
     ]
    }
   ],
   "source": [
    "#Caution: json files not included on Github. Running will result in data loss.\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50dfa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>pulled_time</th>\n",
       "      <th>selftext</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>over_18</th>\n",
       "      <th>id</th>\n",
       "      <th>is_video</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>time_on_reddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cringetopia</td>\n",
       "      <td>I hate this app</td>\n",
       "      <td>Highground69420</td>\n",
       "      <td>6388</td>\n",
       "      <td>2021-12-26 10:25:10</td>\n",
       "      <td>2021-12-26 21:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>image</td>\n",
       "      <td>1630721</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>royzpp</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>38090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OldSchoolCool</td>\n",
       "      <td>Weed bust in 1970s</td>\n",
       "      <td>hugginuu</td>\n",
       "      <td>362</td>\n",
       "      <td>2021-12-26 17:21:40</td>\n",
       "      <td>2021-12-26 21:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>image</td>\n",
       "      <td>15540465</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>rp7ci7</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>13100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Music</td>\n",
       "      <td>Music elitism is getting annoying.</td>\n",
       "      <td>MeldNoFake</td>\n",
       "      <td>3519</td>\n",
       "      <td>2021-12-26 05:37:53</td>\n",
       "      <td>2021-12-26 21:00:00</td>\n",
       "      <td>Yes, you can listen to Pink Floyd, The Beatles...</td>\n",
       "      <td>text</td>\n",
       "      <td>28558871</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>roujf1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>55327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>Covid: Travel misery for tens of thousands wit...</td>\n",
       "      <td>FullAd2253</td>\n",
       "      <td>2620</td>\n",
       "      <td>2021-12-26 07:13:24</td>\n",
       "      <td>2021-12-26 21:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>link</td>\n",
       "      <td>23982213</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>rovth5</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>49596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pics</td>\n",
       "      <td>Hélène Boudreu's actual graduation picture fro...</td>\n",
       "      <td>eliazer1</td>\n",
       "      <td>827</td>\n",
       "      <td>2021-12-26 17:34:00</td>\n",
       "      <td>2021-12-26 21:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>image</td>\n",
       "      <td>28396351</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>rp7lry</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>12360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                              title  \\\n",
       "0    Cringetopia                                    I hate this app   \n",
       "1  OldSchoolCool                                 Weed bust in 1970s   \n",
       "2          Music                 Music elitism is getting annoying.   \n",
       "3           news  Covid: Travel misery for tens of thousands wit...   \n",
       "4           pics  Hélène Boudreu's actual graduation picture fro...   \n",
       "\n",
       "            author  num_comments          created_utc          pulled_time  \\\n",
       "0  Highground69420          6388  2021-12-26 10:25:10  2021-12-26 21:00:00   \n",
       "1         hugginuu           362  2021-12-26 17:21:40  2021-12-26 21:00:00   \n",
       "2       MeldNoFake          3519  2021-12-26 05:37:53  2021-12-26 21:00:00   \n",
       "3       FullAd2253          2620  2021-12-26 07:13:24  2021-12-26 21:00:00   \n",
       "4         eliazer1           827  2021-12-26 17:34:00  2021-12-26 21:00:00   \n",
       "\n",
       "                                            selftext post_hint  \\\n",
       "0                                               None     image   \n",
       "1                                               None     image   \n",
       "2  Yes, you can listen to Pink Floyd, The Beatles...      text   \n",
       "3                                               None      link   \n",
       "4                                               None     image   \n",
       "\n",
       "   subreddit_subscribers  num_crossposts  over_18      id  is_video  \\\n",
       "0                1630721               6     True  royzpp     False   \n",
       "1               15540465               0    False  rp7ci7     False   \n",
       "2               28558871               0    False  roujf1     False   \n",
       "3               23982213               2    False  rovth5     False   \n",
       "4               28396351               5     True  rp7lry     False   \n",
       "\n",
       "   parent_whitelist_status  time_on_reddit  \n",
       "0                        1           38090  \n",
       "1                        3           13100  \n",
       "2                        3           55327  \n",
       "3                        3           49596  \n",
       "4                        3           12360  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/thread_df.csv\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
